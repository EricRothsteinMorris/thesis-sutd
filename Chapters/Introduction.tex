%!TEX root = ../main.tex
% Chapter Template



\chapter{Introduction} % Main chapter title
\label{ch:Introduction} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------\section{Personal Statement}
% Alice: "I've implemented a system that tolerates the presence of any attacker"\\
% % Bob: "Which attacker?"\\
% % Alice: "A trivial attacker; one that does nothing."\\
% Bob: "What does your system do?"\\
% Alice: "Nothing"\\
% Bob: "Oh! That I can believe."\\

% \todo[inline]{TOO GENERAL}
% A \emph{secure system} preserves its integrity, confidentiality and availability in the presence of adversaries~\cite{CIAModel}. 
% %A \emph{robust system} preserves its integrity in the presence of adversaries.
% Not all adversaries are the same; a passive attacker only observes, and while this attacker may not compromise the integrity or the availability of the system, they may compromise its confidentiality. 

% %What does it mean for a system to work in the presence of an adversary?
% \begin{itemize}
%     \item There have been not many uses for coalgebra and security.
%     \item Noninterference is a behavioural property 
% \end{itemize}

% Modern systems must not only be correct, but also secure. Proving that a system is correct is a difficult problem, but I would say that proving that a system is secure is exponentially more difficult. Correctness proofs study properties of the system in isolation or under strong interaction guarantees, i.e., the environment never misbehaves. 
\section{Motivation: Classifying Attackers}
To protect our systems, we need to understand what attackers can do to them. Basin and Cremers~\cite{KnowYourEnemy} quantify the power attackers in security protocols by defining a security protocols hierarchy with respect to a set of attacker models where %introduced \emph{Scyther}~\cite{Scyther}, 
the position of a protocol in the hierarchy depends on its level of assurance with respect to a confidentiality property that the attackers aim to break: protocols higher in the hierarchy can tolerate the presence of more powerful attackers (i.e., attackers with more capabilities). More precisely, the protocol hierarchy is directed acyclic graph whose nodes are pairs of a protocol and a set of attacker capabilities, where two nodes share an edge if the source protocol-set of capabilities satisfies the state property, but adding an additional capabilities would break the state property in the protocol, whilst the property is still satisfied in the destination node despite the addition of those capabilities.

To check if a protocol satisfies a state property (e.g. perfect forward secrecy) in the presence of adversaries, Basin and Cremers model the protocol as symbolic transition system, and they model an adversary as a set of capabilities that enable previously inexistent transitions in the protocol. These new transitions expand the set of reachable states in the protocol, and this new set of reachable states may contain a state where the knowledge of the attacker has enough sensitive information to break the state property being verified. The idea of an attacker adding new transitions to an existing system is simple yet powerful, because it allows us to study exceptional behaviours of the system. However, to which extend can an attacker add new transitions to a system? Can they add arbitrary transitions? Can they also remove existing transitions? All these are pertinent questions, and their answers have interesting implications. Basin and Cremers~\cite{KnowYourEnemy} do not allow attackers to create arbitrary transitions; attackers can only create them based on the following three aspects: \emph{which} kind of data is compromised, \emph{whose} data is it, and \emph{when} the compromise occurs. 

Basin and Cremers illustrate how we can model attackers symbolically as sets of capabilities that abstract the concrete implementation of attacks and focus exclusively on their effects. The semantics of a new transition $a\rightarrow b$ introduced by attackers corresponds to an abstraction of the effect of some attack at the source state $a$ which causes the system to reach a state $b$. In this sense, the attacker has capabilities to change the state of the system. Arbitrary transitions are theoretically possible, but they may be practically infeasible; for example, we could create a transition from a state where the attacker has no knowledge about any secrets to a state where the attacker has knowledge of all of the secrets; such attacker could clearly break any confidentiality protocol, but the realism of such attacker model is questionable. 

\section{Beyond Confidentiality and Symbolic Attackers}
The work by Basin and Cremers verifies protocols with respect to confidentiality properties, but more precisely with respect to safety properties, since it uses a reachability approach. It is possible to study similar safety properties using their method, even if the properties address domains beyond confidentiality, i.e., integrity or availability. We are particularly interested in studying the \emph{robustness} of systems, a concept which is closer to integrity properties than it is to confidentiality.

Robustness is a property related to the integrity of systems in the presence of adversaries. Informally, a \emph{robust system} preserves its integrity in the presence of adversaries; i.e., despite the effect of attacks, the system does not reach any critical states. If we are to use a method similar to the one shown by Basin and Cremers, we need to define attacker models in such a way that these attackers enable new transitions. 

Basin and Cremers describe attackers with symbolic capabilities (e.g., an attacker can reveal a long-term secret key). This simplifies the verification process, since the details of the process of how information is revealed is abstracted away. Unfortunately, due to this high level of abstraction, we do not learn how an attacker can actually obtain those secrets. We are interested not only in quantifying how robust a system is, but also in the concrete strategies that attackers may use when attacking the system. We can achieve both by lowering the level of abstraction of the attacker; i.e., we are going to define attackers as sets of actions instead of a set of assumptions. %Under this formulation, attackers correspond to sets of \emph{spatial transformations}. 

\section{Idea: Attackers as Sets of Spatial Transformations}
A \emph{spatial transformation} is an endofunction in the universe of states of a transition system; i.e. it is a function that maps states to states. We now give an idea of how the the compromise rules of attackers in ~\cite{KnowYourEnemy} can be generalised to spatial transformations. 

In a simple setting, consider the set of real number pairs $\Real\times\Real$ and the successor function $\delta\colon \Real\times\Real\rightarrow \Real\times\Real$ that maps $(x,y)\in \Real$ to $(x^2,y)$. The pair $(\Real\times\Real, \delta)$ defines a transition system, which we represent as a vector space in Figure~\ref{fig:IntroVectorSpace}.
\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{Figures/VectorSpace1.pdf} 
    \caption{Phase space of the transition system $(\Real\times\Real, \delta)$ in the interval $[-20,20]\times[-20,20]$. The behaviour of pairs $(x,y)$ preserves the value of $y$, and the behaviour of $x$ diverges if $|x|>1$.}
    \label{fig:IntroVectorSpace}
  \end{figure}
  \begin{figure}[t]
    \centering  
    \includegraphics[width=\textwidth]{Figures/VectorSpace2.pdf} 
    \caption{Phase space of the spatial transformation $m$ in the interval $[-20,20]\times[-20,20]$.} 
    \label{fig:SpatialDeformation} 
\end{figure} 
The behaviour of $(x,y)\in \Real\times\Real$ is the sequence that results from the iterated application of $\delta$ starting from $(x,y)$; i.e., $(x^2,y),(x^4,y),(x^8,y),$ etc. Now, consider the endofunction $m\colon \Real\times\Real\rightarrow \Real\times\Real$ that maps $(x,y)\in \Real\times\Real$ to $(-y,x)$. The transformation $m$ generates a second vector space, shown in Figure~\ref{fig:SpatialDeformation}. Vector addition is non-commutative in this scenario: applying $\delta$ first and then $m$ maps $(x,y)$ %to $(x/2,y/2)$ and then 
to $(-y,x^2)$ (whose vector space we show in Figure~\ref{fig:FirstLatent}) while applying $m$ first and then $\delta$ maps $(x,y)$ to $(y^2,x)$ (whose vector space we show in Figure~\ref{fig:SecondLatent}). %Since the original transition system is $(\Nat, +_1)$, the effect of $*_2$ models an external perturbation over $(\Nat, +_1)$.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{Figures/VectorSpace4.pdf} 
    \caption{Phase space of the latent behaviour of $(\Real\times\Real, \delta)$ revealed by the spatial transformation $m$ in the interval $[-20,20]\times[-20,20]$.}
    \label{fig:FirstLatent}
\end{figure} 

\begin{figure}[t] 
    \includegraphics[width=\textwidth]{Figures/VectorSpace3.pdf} 
    \caption{Phase space of the dual latent behaviour of the system $(\Real\times\Real, m)$ revealed by $\delta$ in the interval $[-20,20]\times[-20,20]$. It is a rotation of the behaviour shown in Figure~\ref{fig:FirstLatent}.}
    \label{fig:SecondLatent} 
  \end{figure}

Since $(\Real\times\Real, \delta)$ is the current transition system and $m$ is interpreted a spatial transformation, we consider the system shown in Figure~\ref{fig:SecondLatent}, where we apply $m$ first and then $\delta$, mapping $(x,y)$ to $(y^2,x)$. This transformation causes the behaviour of the system to lose the behavioural property of preserving the $y$ value of $(x,y)$ pairs, so if an attacker was capable of applying $m$ to the system $(\Real\times\Real, \delta)$, then they can clearly break the value preservation property. 

%Now, the behaviour of $x\in\Nat$ changes from $x+1,x+2,x+3,x+4,\ldots$ to $2x+1,4x+3,8x+7,16x+15\ldots$. 
%The original system $(\Real\times\Real, \delta)$ not have these behaviours, as they are only revealed after applying $*_2$ by, e.g., an attacker. 
% We can vary the spatial transformation $m\colon \Real\times\Real\rightarrow \Real\times\Real$ to reveal reveals a behaviour that uses the transition function of the system $(\Nat, +_1)$. In the dual case, to apply $+_1$ first and then $*_2$ is equivalent to reveal a new behaviour for the transition system $(\Nat, *_2)$ using $+_1$. These new behaviours are \emph{latent}, since they are only revealed in the presence of a non-trivial spatial transformation. 

In this thesis, we intend to model attackers of a system as sets of spatial transformations, forcing them to reveal new behaviours only by affecting the state space, leaving the transition function intact; in other words, an attacker can produce new denotational semantics, but they cannot affect the operational semantics of the system. This creates a ``game" between attacker and system: the attacker moves first and transforms the initial state, the system then moves by applying its transition function, and then the attacker moves again, transforming the resulting states. The system moves again, and this process iterates. We do not consider cases where the attacker takes two turns in a row or where the system takes two turns in a row, since we can emulate them by an intercalated game. More precisely, an attacker that takes two turns in a row to transform a state $x$ into a state $y$ and then to a state $z$ is equivalent to an attacker that takes only one turn and transforms $x$ into $z$ directly. A system that takes two turns in a row by mapping a state $a$ to a state $b$ and then to a state $c$ is equivalent to a turn-based game between an attacker and system where the attacker used an identity transformation to map $a$ to itself and $b$ to itself. 

We define attacker models by imposing restrictions over the spatial transformations that are available to an attacker. An attacker that can only apply an identity spatial transformation (i.e., one that maps a state to itself) has only trivial capabilities, and fits a passive attacker model that can only watch as the system executes. An attacker that can apply any spatial transformations can manipulate the state space of the transition system as they wish, rendering the dynamics of the system obsolete because they can be bypassed via spatial transformations. However, when we restrict the set of spatial transformations available to an attacker, we observe interesting interactions. For example, the attackers described by Basin and Cremers in ~\cite{KnowYourEnemy} cannot enable arbitrary transitions; instead, there is a set of
%Although this formalism discards attackers that can change the program of the system,  %By protecting the operational semantics of the transition system (i.e., the transition function), we force attackers to follow the rules 
compromise rules that only enable specific state transitions. An attacker model is a set of compromise rules, so more capable attackers have more enabled transitions at their disposal. We interpret the transitions enabled by the compromise rules as a spatial transformation that reveals latent behaviours. 

Not all systems have their operational semantics described in terms of endofunctions. For example, reactive systems require an external input to proceed. To define a unified methodology for the application of spatial transformations in transition systems, we use \emph{universal coalgebra}.


% % Without any restrictions, spatial transformations can map states in any imaginable way.

% From a high-level perspective, we compose the effect of the attacker with the effects of the normal behaviour of the system, and we study the composition to measure robustness and find ways to improve it.

\section{Universal Coalgebra and Spatial Transformations}
To soundly extend the applicability of the ``game" between spatial transformations and deterministic systems to other models of computation, we adopt the framework of \emph{universal coalgebra}~\cite{UniversalCoalgebra}.  
Several authors have used universal coalgebra to generalise results previously exclusive to certain computational models; e.g., the \emph{powerset construction}~\cite{PowersetConstruction}, which is used to discretise a non-deterministic finite automaton, generalises to other computational effects beyond non-determinism~\cite{GeneralisingDetermination}, or e.g., Kleene's Theorem~\cite{KleenesTheorem}, which states that every deterministic finite automaton has an equivalent regular expression, generalises to Mealy machines~\cite{KleeneCoalgebra}.

In universal coalgebra, a dynamical system is often represented as a pair $(X,c)$ where $X$ is a set, called the \emph{carrier}, and $c\colon X\rightarrow F(X)$ is a function where $F(X)$, informally, is the set of the features that the elements of $X$ may have (e.g., continuation, termination, or other computational side effects). Using framework of universal coalgebra, our definitions naturally port from one computational model to another, widening their applicability. 

% % Brzozowski's algorithm for the minimisation of deterministic finite automata can be generalised to deterministic Moore automata~\cite{CoBrzozowski}.

% We would like our definitions of attacker models to be as general as they possibly can so that they apply to as many systems as possible. For that reason, we model systems and attackers using the . 

% Universal coalgebra has also been used for applications in system security. 

\section{Goal of the Thesis and Outline}
\label{sec:Introduction:Goal}
The main objective of the thesis is to illustrate the usefulness and versatility of parametrising the study of transition systems with a spatial transformation that reveals latent behaviours, a process that we call \emph{latent behaviour analysis} (LBA). These latent behaviours capture extraordinary semantics that are realisable by the systems under the effect of state corruption or alterations; using LBA we can systematically study the behaviour of systems in the presence of adversaries or under the influence of property enforcers. We use universal coalgebra to define LBA in an abstract setting, and we illustrate its applicability in three practical scenarios: the classification of attacker models in a reactive system, the quantification and improvement of robustness in cyber-physical systems (CPS), and the enforcement of timing side-channel freedom in LLVM-IR programs. 

We divide this thesis into two major modules: \emph{theory} and \emph{applications}. The theory module presents LBA in the framework of universal coalgebra (Chapter~\ref{ch:LatentBehaviours}), and the application module shows three different instances of the application of LBA to three practical problems: the classification of attacker models in Chapter~\ref{ch:Classification}, the quantification and repair of robustness in CPS in Chapter~\ref{ch:CPSRobustness}, and the enforcement of timing side-channel freedom with respect to memory access patterns for LLVM-IR programs in Chapter~\ref{ch:SideChannelRepair}.

\section{Research Questions and Summary of Contributions}
\label{sec:Introduction:ResearchQuestions}
To accomplish the goal of this thesis, we study the following research questions. When appropriate, we analyse these questions in the context of the application scenarios for LBA that we present in the following chapters. 
\begin{question}
\label{que:AttackerModel}
How do we precisely and systematically describe and generate attacker models, attacks and attackers?
\end{question}
We approach this question by modelling attacker models with sets of capabilities that create spatial transformations that model the attacks available for attackers fitting that model. We discuss this question in detail in Chapters~\ref{ch:LatentBehaviours},~\ref{ch:Classification}, and ~\ref{ch:CPSRobustness}.
\begin{question}
\label{que:Quantification}
How do we efficiently quantify the effects of a set of attacker models attacker fitting a given attacker model on a system? 
\end{question}
The presence of an attacker does not necessarily imply a security risk, e.g., if the attacker can only slightly deviate the behaviour of the system without breaking any security requirements. To address this question, we propose a quantification method that uses bounded model checking in Chapter~\ref{ch:Classification} and that uses testing in Chapter~\ref{ch:CPSRobustness}. We implement the systematic quantification of the effect of attackers via LBA. 

\begin{question}
\label{que:Classification}
How do we compare attacker models and attackers concerning the impact that they potentially have on a given system?
\end{question}
Attackers that have more interaction points with a system are not necessarily the most dangerous. We take inspiration from Basin and Cremers ~\cite{KnowYourEnemy} to approach this question. To determine which attackers are more threatening, we use the quantification methods proposed in Chapter~\ref{ch:Classification} and Chapter~\ref{ch:CPSRobustness} to obtain a partial order of attackers relative to their harmful effects on systems. In particular, thanks to our modelling of attackers via sets of spatial transformations, the partial order of attackers that naturally arises from set inclusion has a monotonicity property for the harmful effects of attackers on a system. We explain this monotonicity property in Chapters~\ref{ch:Classification} and ~\ref{ch:CPSRobustness}.

\begin{question}
\label{que:Repair}
How do we repair a system that is vulnerable to a given attacker model?
\end{question}
Finally, we approach this broad question from two directions: first, we show how LBA can suggest counterattacks for identified attacks in Chapters~\ref{ch:CPSRobustness}, and second, we explain how to use spatial transformations to enforce a security property (i.e., timing side-channel freedom) in Chapter ~\ref{ch:SideChannelRepair}. 

% We do not claim to expand beyond the state of the art in coaglebras, instead, our interest lies primarily on applications for which spatial transformations of transition systems provide an insight or a benefit for an application. 
\section{Related Work}
We now discuss related work regarding the motivation presented in this chapter and the theory presented in Chapter~\ref{ch:LatentBehaviours}. Each chapter on the applications module, i.e. Chapters contains its own related work section.
\todo[inline]{Small introduction to the works mentioned before in coalgebra and some works that use coalgebra and security, like the one that Jorge told me...}


% {\color{blue}
% \subsection{Universal Coalgebra and Security}
% \todo[inline]{Write about kleene coalgebra and other applications }
% \todo[inline]{Heap}


% %; in other words, the attackers can \emph{corrupt} the current state of systems.



% Although the work of Basin and Cremers addresses a confidentiality issue, 

% How do we define realistic attacker models for transition systems? Perhaps it is easier if we start with a system as simple as possible. 
% \todo[inline]{maybe something on fault analysis? I dunno.}


% %If an attacker can arbitrarily change the transition structure of a system, is it possible to defend against such attacker? 
% %It is an idea compatible with both model checking and testing, which is how we can obtain hierarchies of systems. 
% %While Basin and Cremers study the effect of attackers for confidentiality, we are interested in studying the effect of attackers for integrity. Clarkson and Schneider write~\cite{QuantitativeIntegrity} 


% %  span over three dimensions: \emph{which} data is compromised, \emph{whose} data is it, and \emph{when} the compromise occurs. In a nutshell, a they model a protocol 

% % %Attackers that are powerful can replicate the actions of weaker attackers, 

% % Pairing systems and attacker models to obtain a position



% % In particular, \emph{cyber-physical systems} (CPS), which comprise a majority of the critical infrastructure in many countries, 

% % \emph{Universal coalgebra}~\cite{UniversalCoalgebra} 






% \todo[inline]{Biba Duality}
% \todo[inline]{Basin Cremers attackers as sets of effects KYE}
% \todo[inline]{Confidentiality: test if attackers have some knowledge; Integrity: test if the system is too corrupted.}
% \todo[inline]{What, when, how? Modelling attacks}
% \todo[inline]{if we know what attacker models are mathematically, we can ask the computer to do analysis for us in integrity. }
% \todo[inline]{YOU HAVE TO ADDRESS the problem for confidentiality and cite relevant papers, and quantificaiton}
% \todo[inline]{What does Unifying Facets of Information Integrity say?}

% \todo[inline]{The idea is that there is a lot of stuff, but they ultimately belong to a bigger set of ``program correctness''. We argue that those are behavioural properties}

% \todo[inline]{Coalgebras provide a clear view of the set of behavioural properties for families of systems. That makes them really attractive for studying. Not only that, they offer generalised processes for the incorporation of computational effects (e.g., non-determinism, exceptions, termination, etc.) because of category theory.}

% Probably the most important is to motivate the quantification of attackers and how it gave rise to the general treatment of latent behaviours.
% \todo[inline]{}
% \todo[inline]{The first thing we need to do is describe why latent behaviours are interesting. This requires a historical context. }
% \section{Coalgebras}
% \todo[inline]{This section explains the importance of coalgebras as a modelling system, why they are so compatible with computability (monads), and in general why they are useful.}
% \todo[inline]{I could probably be inspired on how to structure this section by publications on coalgebras. There is a large body of literature I can use.}
% \todo[inline]{At least cover that systems are of a form $\delta\colon X\rightarrow D$ where $D$ is some domain, and that there is a way to go from $D$ back to $X$ to continue computation.}
% \todo[inline]{Final coalgebras are important because $D=X$, so it is obvious how to continue.}

% \section{Latent Behaviours}
% \todo[inline]{Explain that in this document we explore the notion of creating new behaviours through transformations of the state space of systems.}
% \subsection{Intuition}
% \todo[inline]{We present a light version of latent behaviours: you normally start at $X$, then you go to $D$ and then back to $X$, but what if we transformed $X$?}
% In latent behaviours we use a transformation $X\rightarrow X$ to affect the behaviour of the system. This transformation is an abstraction; the original system naturally transforms to a system $X\rightarrow D \rightarrow X$ using composition, also of type $X\rightarrow X$. Each function $X\rightarrow X$ can be seen as a black-box of some behaviour. Their interweaving means that they are acting simultaneously. 



% We do not pretend to solve these questions at this broad level, but we address concrete instances of these questions, which appear as we instantiate systems, attacker models, security requirements, etc. in the following sections.
% \section{Applications}
% \subsection{Attacker Classification}
% How can we model attackers? How can we model their actions? How can we prepare against them?
% \subsection{Cyber-Physical System Redesign}
% Many attacks are discovered through testing: an experienced hacker creates a proof of concept to illustrate how some system may be attacked. Then, an abstraction is created to explain why the attack works, and to suggest countermeasures against it. 
% \subsection{Program Repair}



%Fuzzing is probably one of the most popular techniques for system transformations. 

% \todo[inline]{Actually, the thesis would be super interesting if potential behaviour problems are presented in terms of a game between the system and the attacker, with the attacker following some sort of reactive strategy, and the system as well. At some point, one or the other wins because it is a deterministic game if the system is deterministic. }



% \todo[inline]{You can probably show several papers that display this pattern.} 

